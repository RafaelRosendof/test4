{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, TrainerCallback,ProgressCallback\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafaelrosendo/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:1362: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo e o tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
    "\n",
    "# Definir o comprimento m√°ximo de sequ√™ncia para 1024 tokens\n",
    "tokenizer.model_max_length = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar seus dados do CSV (substitua o valor de 'path_to_csv' pelo caminho do seu arquivo CSV)\n",
    "path_to_csv = \"/home/rafaelrosendo/gpt2/portuguese-poems.csv\"\n",
    "data = pd.read_csv(path_to_csv)[\"Content\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embaralhar a lista de dados para garantir uma divis√£o aleat√≥ria\n",
    "random.shuffle(data)\n",
    "\n",
    "# Definir a propor√ß√£o do conjunto de valida√ß√£o (20%)\n",
    "validation_ratio = 0.2\n",
    "\n",
    "# Calcular o √≠ndice para dividir os dados\n",
    "split_index = int(len(data) * (1 - validation_ratio))\n",
    "\n",
    "# Dividir os dados em treinamento e valida√ß√£o\n",
    "train_data = data[:split_index]\n",
    "validation_data = data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafaelrosendo/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Converter os dados em um formato adequado para o treinamento do modelo\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=path_to_csv,\n",
    "    block_size=128,\n",
    ")\n",
    "validation_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=path_to_csv,\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar os argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_fine_tuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.trainer_callback.ProgressCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "ProgressCallback\n",
      "/home/rafaelrosendo/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4723bea675d4b44a3628727ffd7382c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5642a76cfb4227bb4e04c92d606b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/402460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8952, 'learning_rate': 4.9937882025542915e-05, 'epoch': 0.01}\n",
      "{'loss': 5.8952, 'learning_rate': 4.9937882025542915e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7537, 'learning_rate': 4.987576405108583e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7537, 'learning_rate': 4.987576405108583e-05, 'epoch': 0.01}\n",
      "{'loss': 5.7623, 'learning_rate': 4.981364607662873e-05, 'epoch': 0.02}\n",
      "{'loss': 5.7623, 'learning_rate': 4.981364607662873e-05, 'epoch': 0.02}\n",
      "{'loss': 5.6725, 'learning_rate': 4.9751528102171645e-05, 'epoch': 0.02}\n",
      "{'loss': 5.6725, 'learning_rate': 4.9751528102171645e-05, 'epoch': 0.02}\n",
      "{'loss': 5.647, 'learning_rate': 4.968941012771456e-05, 'epoch': 0.03}\n",
      "{'loss': 5.647, 'learning_rate': 4.968941012771456e-05, 'epoch': 0.03}\n",
      "{'loss': 5.6499, 'learning_rate': 4.962729215325747e-05, 'epoch': 0.04}\n",
      "{'loss': 5.6499, 'learning_rate': 4.962729215325747e-05, 'epoch': 0.04}\n",
      "{'loss': 5.5911, 'learning_rate': 4.9565174178800375e-05, 'epoch': 0.04}\n",
      "{'loss': 5.5911, 'learning_rate': 4.9565174178800375e-05, 'epoch': 0.04}\n",
      "{'loss': 5.5382, 'learning_rate': 4.950305620434329e-05, 'epoch': 0.05}\n",
      "{'loss': 5.5382, 'learning_rate': 4.950305620434329e-05, 'epoch': 0.05}\n",
      "{'loss': 5.6073, 'learning_rate': 4.94409382298862e-05, 'epoch': 0.06}\n",
      "{'loss': 5.6073, 'learning_rate': 4.94409382298862e-05, 'epoch': 0.06}\n",
      "{'loss': 5.4947, 'learning_rate': 4.937882025542912e-05, 'epoch': 0.06}\n",
      "{'loss': 5.4947, 'learning_rate': 4.937882025542912e-05, 'epoch': 0.06}\n",
      "{'loss': 5.4441, 'learning_rate': 4.931670228097203e-05, 'epoch': 0.07}\n",
      "{'loss': 5.4441, 'learning_rate': 4.931670228097203e-05, 'epoch': 0.07}\n",
      "{'loss': 5.5171, 'learning_rate': 4.9254584306514936e-05, 'epoch': 0.07}\n",
      "{'loss': 5.5171, 'learning_rate': 4.9254584306514936e-05, 'epoch': 0.07}\n",
      "{'loss': 5.4592, 'learning_rate': 4.919246633205785e-05, 'epoch': 0.08}\n",
      "{'loss': 5.4592, 'learning_rate': 4.919246633205785e-05, 'epoch': 0.08}\n",
      "{'loss': 5.4751, 'learning_rate': 4.913034835760076e-05, 'epoch': 0.09}\n",
      "{'loss': 5.4751, 'learning_rate': 4.913034835760076e-05, 'epoch': 0.09}\n",
      "{'loss': 5.4712, 'learning_rate': 4.906823038314367e-05, 'epoch': 0.09}\n",
      "{'loss': 5.4712, 'learning_rate': 4.906823038314367e-05, 'epoch': 0.09}\n",
      "{'loss': 5.4484, 'learning_rate': 4.900611240868658e-05, 'epoch': 0.1}\n",
      "{'loss': 5.4484, 'learning_rate': 4.900611240868658e-05, 'epoch': 0.1}\n",
      "{'loss': 5.4348, 'learning_rate': 4.894399443422949e-05, 'epoch': 0.11}\n",
      "{'loss': 5.4348, 'learning_rate': 4.894399443422949e-05, 'epoch': 0.11}\n",
      "{'loss': 5.4519, 'learning_rate': 4.88818764597724e-05, 'epoch': 0.11}\n",
      "{'loss': 5.4519, 'learning_rate': 4.88818764597724e-05, 'epoch': 0.11}\n",
      "{'loss': 5.4457, 'learning_rate': 4.8819758485315315e-05, 'epoch': 0.12}\n",
      "{'loss': 5.4457, 'learning_rate': 4.8819758485315315e-05, 'epoch': 0.12}\n",
      "{'loss': 5.4212, 'learning_rate': 4.875764051085823e-05, 'epoch': 0.12}\n",
      "{'loss': 5.4212, 'learning_rate': 4.875764051085823e-05, 'epoch': 0.12}\n",
      "{'loss': 5.3932, 'learning_rate': 4.869552253640113e-05, 'epoch': 0.13}\n",
      "{'loss': 5.3932, 'learning_rate': 4.869552253640113e-05, 'epoch': 0.13}\n",
      "{'loss': 5.4469, 'learning_rate': 4.8633404561944044e-05, 'epoch': 0.14}\n",
      "{'loss': 5.4469, 'learning_rate': 4.8633404561944044e-05, 'epoch': 0.14}\n",
      "{'loss': 5.3072, 'learning_rate': 4.857128658748696e-05, 'epoch': 0.14}\n",
      "{'loss': 5.3072, 'learning_rate': 4.857128658748696e-05, 'epoch': 0.14}\n",
      "{'loss': 5.371, 'learning_rate': 4.850916861302987e-05, 'epoch': 0.15}\n",
      "{'loss': 5.371, 'learning_rate': 4.850916861302987e-05, 'epoch': 0.15}\n",
      "{'loss': 5.3222, 'learning_rate': 4.8447050638572774e-05, 'epoch': 0.16}\n",
      "{'loss': 5.3222, 'learning_rate': 4.8447050638572774e-05, 'epoch': 0.16}\n",
      "{'loss': 5.3312, 'learning_rate': 4.838493266411569e-05, 'epoch': 0.16}\n",
      "{'loss': 5.3312, 'learning_rate': 4.838493266411569e-05, 'epoch': 0.16}\n",
      "{'loss': 5.2938, 'learning_rate': 4.83228146896586e-05, 'epoch': 0.17}\n",
      "{'loss': 5.2938, 'learning_rate': 4.83228146896586e-05, 'epoch': 0.17}\n",
      "{'loss': 5.3567, 'learning_rate': 4.826069671520151e-05, 'epoch': 0.17}\n",
      "{'loss': 5.3567, 'learning_rate': 4.826069671520151e-05, 'epoch': 0.17}\n",
      "{'loss': 5.3034, 'learning_rate': 4.819857874074442e-05, 'epoch': 0.18}\n",
      "{'loss': 5.3034, 'learning_rate': 4.819857874074442e-05, 'epoch': 0.18}\n",
      "{'loss': 5.3397, 'learning_rate': 4.8136460766287336e-05, 'epoch': 0.19}\n",
      "{'loss': 5.3397, 'learning_rate': 4.8136460766287336e-05, 'epoch': 0.19}\n",
      "{'loss': 5.3133, 'learning_rate': 4.807434279183025e-05, 'epoch': 0.19}\n",
      "{'loss': 5.3133, 'learning_rate': 4.807434279183025e-05, 'epoch': 0.19}\n",
      "{'loss': 5.292, 'learning_rate': 4.801222481737316e-05, 'epoch': 0.2}\n",
      "{'loss': 5.292, 'learning_rate': 4.801222481737316e-05, 'epoch': 0.2}\n",
      "{'loss': 5.2628, 'learning_rate': 4.795010684291607e-05, 'epoch': 0.2}\n",
      "{'loss': 5.2628, 'learning_rate': 4.795010684291607e-05, 'epoch': 0.2}\n",
      "{'loss': 5.2462, 'learning_rate': 4.788798886845898e-05, 'epoch': 0.21}\n",
      "{'loss': 5.2462, 'learning_rate': 4.788798886845898e-05, 'epoch': 0.21}\n",
      "{'loss': 5.2642, 'learning_rate': 4.782587089400189e-05, 'epoch': 0.22}\n",
      "{'loss': 5.2642, 'learning_rate': 4.782587089400189e-05, 'epoch': 0.22}\n",
      "{'loss': 5.2324, 'learning_rate': 4.77637529195448e-05, 'epoch': 0.22}\n",
      "{'loss': 5.2324, 'learning_rate': 4.77637529195448e-05, 'epoch': 0.22}\n",
      "{'loss': 5.2567, 'learning_rate': 4.7701634945087714e-05, 'epoch': 0.23}\n",
      "{'loss': 5.2567, 'learning_rate': 4.7701634945087714e-05, 'epoch': 0.23}\n",
      "{'loss': 5.2823, 'learning_rate': 4.763951697063063e-05, 'epoch': 0.24}\n",
      "{'loss': 5.2823, 'learning_rate': 4.763951697063063e-05, 'epoch': 0.24}\n",
      "{'loss': 5.2617, 'learning_rate': 4.757739899617353e-05, 'epoch': 0.24}\n",
      "{'loss': 5.2617, 'learning_rate': 4.757739899617353e-05, 'epoch': 0.24}\n",
      "{'loss': 5.2355, 'learning_rate': 4.7515281021716444e-05, 'epoch': 0.25}\n",
      "{'loss': 5.2355, 'learning_rate': 4.7515281021716444e-05, 'epoch': 0.25}\n",
      "{'loss': 5.2618, 'learning_rate': 4.745316304725936e-05, 'epoch': 0.25}\n",
      "{'loss': 5.2618, 'learning_rate': 4.745316304725936e-05, 'epoch': 0.25}\n",
      "{'loss': 5.2779, 'learning_rate': 4.739104507280227e-05, 'epoch': 0.26}\n",
      "{'loss': 5.2779, 'learning_rate': 4.739104507280227e-05, 'epoch': 0.26}\n",
      "{'loss': 5.2557, 'learning_rate': 4.7328927098345174e-05, 'epoch': 0.27}\n",
      "{'loss': 5.2557, 'learning_rate': 4.7328927098345174e-05, 'epoch': 0.27}\n",
      "{'loss': 5.2637, 'learning_rate': 4.7266809123888087e-05, 'epoch': 0.27}\n",
      "{'loss': 5.2637, 'learning_rate': 4.7266809123888087e-05, 'epoch': 0.27}\n",
      "{'loss': 5.2288, 'learning_rate': 4.7204691149431e-05, 'epoch': 0.28}\n",
      "{'loss': 5.2288, 'learning_rate': 4.7204691149431e-05, 'epoch': 0.28}\n",
      "{'loss': 5.1742, 'learning_rate': 4.714257317497391e-05, 'epoch': 0.29}\n",
      "{'loss': 5.1742, 'learning_rate': 4.714257317497391e-05, 'epoch': 0.29}\n",
      "{'loss': 5.2314, 'learning_rate': 4.708045520051682e-05, 'epoch': 0.29}\n",
      "{'loss': 5.2314, 'learning_rate': 4.708045520051682e-05, 'epoch': 0.29}\n",
      "{'loss': 5.1739, 'learning_rate': 4.7018337226059736e-05, 'epoch': 0.3}\n",
      "{'loss': 5.1739, 'learning_rate': 4.7018337226059736e-05, 'epoch': 0.3}\n",
      "{'loss': 5.2149, 'learning_rate': 4.695621925160265e-05, 'epoch': 0.3}\n",
      "{'loss': 5.2149, 'learning_rate': 4.695621925160265e-05, 'epoch': 0.3}\n",
      "{'loss': 5.231, 'learning_rate': 4.689410127714556e-05, 'epoch': 0.31}\n",
      "{'loss': 5.231, 'learning_rate': 4.689410127714556e-05, 'epoch': 0.31}\n",
      "{'loss': 5.1679, 'learning_rate': 4.683198330268847e-05, 'epoch': 0.32}\n",
      "{'loss': 5.1679, 'learning_rate': 4.683198330268847e-05, 'epoch': 0.32}\n",
      "{'loss': 5.2373, 'learning_rate': 4.676986532823138e-05, 'epoch': 0.32}\n",
      "{'loss': 5.2373, 'learning_rate': 4.676986532823138e-05, 'epoch': 0.32}\n",
      "{'loss': 5.1844, 'learning_rate': 4.670774735377429e-05, 'epoch': 0.33}\n",
      "{'loss': 5.1844, 'learning_rate': 4.670774735377429e-05, 'epoch': 0.33}\n",
      "{'loss': 5.21, 'learning_rate': 4.66456293793172e-05, 'epoch': 0.34}\n",
      "{'loss': 5.21, 'learning_rate': 4.66456293793172e-05, 'epoch': 0.34}\n",
      "{'loss': 5.2202, 'learning_rate': 4.6583511404860114e-05, 'epoch': 0.34}\n",
      "{'loss': 5.2202, 'learning_rate': 4.6583511404860114e-05, 'epoch': 0.34}\n",
      "{'loss': 5.1986, 'learning_rate': 4.652139343040303e-05, 'epoch': 0.35}\n",
      "{'loss': 5.1986, 'learning_rate': 4.652139343040303e-05, 'epoch': 0.35}\n",
      "{'loss': 5.1928, 'learning_rate': 4.645927545594593e-05, 'epoch': 0.35}\n",
      "{'loss': 5.1928, 'learning_rate': 4.645927545594593e-05, 'epoch': 0.35}\n",
      "{'loss': 5.1613, 'learning_rate': 4.6397157481488844e-05, 'epoch': 0.36}\n",
      "{'loss': 5.1613, 'learning_rate': 4.6397157481488844e-05, 'epoch': 0.36}\n",
      "{'loss': 5.1446, 'learning_rate': 4.6335039507031757e-05, 'epoch': 0.37}\n",
      "{'loss': 5.1446, 'learning_rate': 4.6335039507031757e-05, 'epoch': 0.37}\n",
      "{'loss': 5.1973, 'learning_rate': 4.627292153257467e-05, 'epoch': 0.37}\n",
      "{'loss': 5.1973, 'learning_rate': 4.627292153257467e-05, 'epoch': 0.37}\n",
      "{'loss': 5.1691, 'learning_rate': 4.6210803558117574e-05, 'epoch': 0.38}\n",
      "{'loss': 5.1691, 'learning_rate': 4.6210803558117574e-05, 'epoch': 0.38}\n",
      "{'loss': 5.1017, 'learning_rate': 4.6148685583660487e-05, 'epoch': 0.39}\n",
      "{'loss': 5.1017, 'learning_rate': 4.6148685583660487e-05, 'epoch': 0.39}\n",
      "{'loss': 5.223, 'learning_rate': 4.60865676092034e-05, 'epoch': 0.39}\n",
      "{'loss': 5.223, 'learning_rate': 4.60865676092034e-05, 'epoch': 0.39}\n",
      "{'loss': 5.1853, 'learning_rate': 4.602444963474631e-05, 'epoch': 0.4}\n",
      "{'loss': 5.1853, 'learning_rate': 4.602444963474631e-05, 'epoch': 0.4}\n",
      "{'loss': 5.1597, 'learning_rate': 4.596233166028922e-05, 'epoch': 0.4}\n",
      "{'loss': 5.1597, 'learning_rate': 4.596233166028922e-05, 'epoch': 0.4}\n",
      "{'loss': 5.1079, 'learning_rate': 4.5900213685832135e-05, 'epoch': 0.41}\n",
      "{'loss': 5.1079, 'learning_rate': 4.5900213685832135e-05, 'epoch': 0.41}\n",
      "{'loss': 5.181, 'learning_rate': 4.583809571137505e-05, 'epoch': 0.42}\n",
      "{'loss': 5.181, 'learning_rate': 4.583809571137505e-05, 'epoch': 0.42}\n",
      "{'loss': 5.1642, 'learning_rate': 4.577597773691796e-05, 'epoch': 0.42}\n",
      "{'loss': 5.1642, 'learning_rate': 4.577597773691796e-05, 'epoch': 0.42}\n",
      "{'loss': 5.1776, 'learning_rate': 4.571385976246087e-05, 'epoch': 0.43}\n",
      "{'loss': 5.1776, 'learning_rate': 4.571385976246087e-05, 'epoch': 0.43}\n",
      "{'loss': 5.1194, 'learning_rate': 4.565174178800378e-05, 'epoch': 0.43}\n",
      "{'loss': 5.1194, 'learning_rate': 4.565174178800378e-05, 'epoch': 0.43}\n",
      "{'loss': 5.1211, 'learning_rate': 4.558962381354669e-05, 'epoch': 0.44}\n",
      "{'loss': 5.1211, 'learning_rate': 4.558962381354669e-05, 'epoch': 0.44}\n",
      "{'loss': 5.1441, 'learning_rate': 4.55275058390896e-05, 'epoch': 0.45}\n",
      "{'loss': 5.1441, 'learning_rate': 4.55275058390896e-05, 'epoch': 0.45}\n",
      "{'loss': 5.1443, 'learning_rate': 4.5465387864632514e-05, 'epoch': 0.45}\n",
      "{'loss': 5.1443, 'learning_rate': 4.5465387864632514e-05, 'epoch': 0.45}\n",
      "{'loss': 5.1824, 'learning_rate': 4.5403269890175427e-05, 'epoch': 0.46}\n",
      "{'loss': 5.1824, 'learning_rate': 4.5403269890175427e-05, 'epoch': 0.46}\n",
      "{'loss': 5.15, 'learning_rate': 4.534115191571833e-05, 'epoch': 0.47}\n",
      "{'loss': 5.15, 'learning_rate': 4.534115191571833e-05, 'epoch': 0.47}\n",
      "{'loss': 5.1014, 'learning_rate': 4.5279033941261244e-05, 'epoch': 0.47}\n",
      "{'loss': 5.1014, 'learning_rate': 4.5279033941261244e-05, 'epoch': 0.47}\n",
      "{'loss': 5.1723, 'learning_rate': 4.5216915966804157e-05, 'epoch': 0.48}\n",
      "{'loss': 5.1723, 'learning_rate': 4.5216915966804157e-05, 'epoch': 0.48}\n",
      "{'loss': 5.1353, 'learning_rate': 4.515479799234707e-05, 'epoch': 0.48}\n",
      "{'loss': 5.1353, 'learning_rate': 4.515479799234707e-05, 'epoch': 0.48}\n",
      "{'loss': 5.1099, 'learning_rate': 4.5092680017889974e-05, 'epoch': 0.49}\n",
      "{'loss': 5.1099, 'learning_rate': 4.5092680017889974e-05, 'epoch': 0.49}\n",
      "{'loss': 5.1775, 'learning_rate': 4.5030562043432886e-05, 'epoch': 0.5}\n",
      "{'loss': 5.1775, 'learning_rate': 4.5030562043432886e-05, 'epoch': 0.5}\n",
      "{'loss': 5.1044, 'learning_rate': 4.49684440689758e-05, 'epoch': 0.5}\n",
      "{'loss': 5.1044, 'learning_rate': 4.49684440689758e-05, 'epoch': 0.5}\n",
      "{'loss': 5.1098, 'learning_rate': 4.490632609451871e-05, 'epoch': 0.51}\n",
      "{'loss': 5.1098, 'learning_rate': 4.490632609451871e-05, 'epoch': 0.51}\n",
      "{'loss': 5.1177, 'learning_rate': 4.484420812006162e-05, 'epoch': 0.52}\n",
      "{'loss': 5.1177, 'learning_rate': 4.484420812006162e-05, 'epoch': 0.52}\n",
      "{'loss': 5.1086, 'learning_rate': 4.4782090145604535e-05, 'epoch': 0.52}\n",
      "{'loss': 5.1086, 'learning_rate': 4.4782090145604535e-05, 'epoch': 0.52}\n",
      "{'loss': 5.1144, 'learning_rate': 4.471997217114745e-05, 'epoch': 0.53}\n",
      "{'loss': 5.1144, 'learning_rate': 4.471997217114745e-05, 'epoch': 0.53}\n",
      "{'loss': 5.0951, 'learning_rate': 4.465785419669036e-05, 'epoch': 0.53}\n",
      "{'loss': 5.0951, 'learning_rate': 4.465785419669036e-05, 'epoch': 0.53}\n",
      "{'loss': 5.1584, 'learning_rate': 4.459573622223327e-05, 'epoch': 0.54}\n",
      "{'loss': 5.1584, 'learning_rate': 4.459573622223327e-05, 'epoch': 0.54}\n",
      "{'loss': 5.1191, 'learning_rate': 4.453361824777618e-05, 'epoch': 0.55}\n",
      "{'loss': 5.1191, 'learning_rate': 4.453361824777618e-05, 'epoch': 0.55}\n",
      "{'loss': 5.1316, 'learning_rate': 4.447150027331909e-05, 'epoch': 0.55}\n",
      "{'loss': 5.1316, 'learning_rate': 4.447150027331909e-05, 'epoch': 0.55}\n",
      "{'loss': 5.1642, 'learning_rate': 4.4409382298862e-05, 'epoch': 0.56}\n",
      "{'loss': 5.1642, 'learning_rate': 4.4409382298862e-05, 'epoch': 0.56}\n",
      "{'loss': 5.1016, 'learning_rate': 4.4347264324404914e-05, 'epoch': 0.57}\n",
      "{'loss': 5.1016, 'learning_rate': 4.4347264324404914e-05, 'epoch': 0.57}\n",
      "{'loss': 5.1217, 'learning_rate': 4.428514634994782e-05, 'epoch': 0.57}\n",
      "{'loss': 5.1217, 'learning_rate': 4.428514634994782e-05, 'epoch': 0.57}\n",
      "{'loss': 5.0544, 'learning_rate': 4.422302837549073e-05, 'epoch': 0.58}\n",
      "{'loss': 5.0544, 'learning_rate': 4.422302837549073e-05, 'epoch': 0.58}\n",
      "{'loss': 5.046, 'learning_rate': 4.4160910401033644e-05, 'epoch': 0.58}\n",
      "{'loss': 5.046, 'learning_rate': 4.4160910401033644e-05, 'epoch': 0.58}\n",
      "{'loss': 5.1445, 'learning_rate': 4.4098792426576556e-05, 'epoch': 0.59}\n",
      "{'loss': 5.1445, 'learning_rate': 4.4098792426576556e-05, 'epoch': 0.59}\n",
      "{'loss': 5.059, 'learning_rate': 4.403667445211947e-05, 'epoch': 0.6}\n",
      "{'loss': 5.059, 'learning_rate': 4.403667445211947e-05, 'epoch': 0.6}\n",
      "{'loss': 5.0779, 'learning_rate': 4.3974556477662374e-05, 'epoch': 0.6}\n",
      "{'loss': 5.0779, 'learning_rate': 4.3974556477662374e-05, 'epoch': 0.6}\n",
      "{'loss': 5.0758, 'learning_rate': 4.3912438503205286e-05, 'epoch': 0.61}\n",
      "{'loss': 5.0758, 'learning_rate': 4.3912438503205286e-05, 'epoch': 0.61}\n",
      "{'loss': 5.0232, 'learning_rate': 4.38503205287482e-05, 'epoch': 0.61}\n",
      "{'loss': 5.0232, 'learning_rate': 4.38503205287482e-05, 'epoch': 0.61}\n",
      "{'loss': 5.048, 'learning_rate': 4.378820255429111e-05, 'epoch': 0.62}\n",
      "{'loss': 5.048, 'learning_rate': 4.378820255429111e-05, 'epoch': 0.62}\n",
      "{'loss': 5.0481, 'learning_rate': 4.372608457983402e-05, 'epoch': 0.63}\n",
      "{'loss': 5.0481, 'learning_rate': 4.372608457983402e-05, 'epoch': 0.63}\n",
      "{'loss': 5.1203, 'learning_rate': 4.3663966605376935e-05, 'epoch': 0.63}\n",
      "{'loss': 5.1203, 'learning_rate': 4.3663966605376935e-05, 'epoch': 0.63}\n",
      "{'loss': 5.0636, 'learning_rate': 4.360184863091985e-05, 'epoch': 0.64}\n",
      "{'loss': 5.0636, 'learning_rate': 4.360184863091985e-05, 'epoch': 0.64}\n",
      "{'loss': 5.079, 'learning_rate': 4.353973065646276e-05, 'epoch': 0.65}\n",
      "{'loss': 5.079, 'learning_rate': 4.353973065646276e-05, 'epoch': 0.65}\n",
      "{'loss': 5.0775, 'learning_rate': 4.347761268200567e-05, 'epoch': 0.65}\n",
      "{'loss': 5.0775, 'learning_rate': 4.347761268200567e-05, 'epoch': 0.65}\n",
      "{'loss': 5.1092, 'learning_rate': 4.341549470754858e-05, 'epoch': 0.66}\n",
      "{'loss': 5.1092, 'learning_rate': 4.341549470754858e-05, 'epoch': 0.66}\n",
      "{'loss': 5.038, 'learning_rate': 4.335337673309149e-05, 'epoch': 0.66}\n",
      "{'loss': 5.038, 'learning_rate': 4.335337673309149e-05, 'epoch': 0.66}\n",
      "{'loss': 5.0552, 'learning_rate': 4.32912587586344e-05, 'epoch': 0.67}\n",
      "{'loss': 5.0552, 'learning_rate': 4.32912587586344e-05, 'epoch': 0.67}\n",
      "{'loss': 5.048, 'learning_rate': 4.3229140784177314e-05, 'epoch': 0.68}\n",
      "{'loss': 5.048, 'learning_rate': 4.3229140784177314e-05, 'epoch': 0.68}\n",
      "{'loss': 5.058, 'learning_rate': 4.316702280972022e-05, 'epoch': 0.68}\n",
      "{'loss': 5.058, 'learning_rate': 4.316702280972022e-05, 'epoch': 0.68}\n",
      "{'loss': 5.013, 'learning_rate': 4.310490483526313e-05, 'epoch': 0.69}\n",
      "{'loss': 5.013, 'learning_rate': 4.310490483526313e-05, 'epoch': 0.69}\n",
      "{'loss': 5.0648, 'learning_rate': 4.3042786860806044e-05, 'epoch': 0.7}\n",
      "{'loss': 5.0648, 'learning_rate': 4.3042786860806044e-05, 'epoch': 0.7}\n",
      "{'loss': 5.1206, 'learning_rate': 4.2980668886348956e-05, 'epoch': 0.7}\n",
      "{'loss': 5.1206, 'learning_rate': 4.2980668886348956e-05, 'epoch': 0.7}\n",
      "{'loss': 5.0574, 'learning_rate': 4.291855091189187e-05, 'epoch': 0.71}\n",
      "{'loss': 5.0574, 'learning_rate': 4.291855091189187e-05, 'epoch': 0.71}\n",
      "{'loss': 5.0954, 'learning_rate': 4.2856432937434774e-05, 'epoch': 0.71}\n",
      "{'loss': 5.0954, 'learning_rate': 4.2856432937434774e-05, 'epoch': 0.71}\n",
      "{'loss': 5.1012, 'learning_rate': 4.2794314962977686e-05, 'epoch': 0.72}\n",
      "{'loss': 5.1012, 'learning_rate': 4.2794314962977686e-05, 'epoch': 0.72}\n",
      "{'loss': 5.0867, 'learning_rate': 4.27321969885206e-05, 'epoch': 0.73}\n",
      "{'loss': 5.0867, 'learning_rate': 4.27321969885206e-05, 'epoch': 0.73}\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o Trainer e iniciar o treinamento\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    callbacks=[ProgressCallback()]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
