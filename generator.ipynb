{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, TrainerCallback,ProgressCallback\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafaelrosendo/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:1362: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo e o tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
    "\n",
    "# Definir o comprimento m√°ximo de sequ√™ncia para 1024 tokens\n",
    "tokenizer.model_max_length = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar seus dados do CSV (substitua o valor de 'path_to_csv' pelo caminho do seu arquivo CSV)\n",
    "path_to_csv = \"/home/rafaelrosendo/gpt2/portuguese-poems.csv\"\n",
    "data = pd.read_csv(path_to_csv)[\"Content\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embaralhar a lista de dados para garantir uma divis√£o aleat√≥ria\n",
    "random.shuffle(data)\n",
    "\n",
    "# Definir a propor√ß√£o do conjunto de valida√ß√£o\n",
    "validation_ratio = 0.2\n",
    "\n",
    "# Calcular o √≠ndice para dividir os dados\n",
    "split_index = int(len(data) * (1 - validation_ratio))\n",
    "\n",
    "# Dividir os dados em treinamento e valida√ß√£o\n",
    "train_data = data[:split_index]\n",
    "validation_data = data[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafaelrosendo/anaconda3/envs/torch2/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Converter os dados em um formato adequado para o treinamento do modelo\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=path_to_csv,\n",
    "    block_size=128,\n",
    ")\n",
    "validation_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=path_to_csv,\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar os argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_fine_tuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o Trainer e iniciar o treinamento\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    callbacks=[ProgressCallback()]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "O mar salgado quanto de teu sal n√£o s√£o l√°grimas de Portugal, mas l√°grimas de um povo que tem vivido em paz. A l√°grimas n√£o podem mais vir a falar para mim, nem me ser capaz de ser ouvida. Os portugueses v√£o falar para mim, eu orei e ele tenho o direito de falar em Portugal para mim.\"\n",
      "\n",
      "\"N√£o sei se eu terei de ser o pr√≥ximo rei. Eu terei de um rei n√£o nascido fora do mundo, n√£o por Deus\".\n",
      "\n",
      "\"Voc√™s sabem que foi meu soberano. √â dif√≠cil conseguir me fazer um rei que n√£o seja de minha consci√™ncia e sem o apoio de Deus\"\n",
      "\n",
      "\"A grande esperan√ßa da minha pessoa √© que ele me d√™ a tudo que ele, que √© necess√°rio\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_text(model, tokenizer, input_text, max_length=50, top_k=40):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    sample_outputs = model.generate(inputs.input_ids,\n",
    "                                    pad_token_id=tokenizer.eos_token_id,\n",
    "                                    do_sample=True, \n",
    "                                    max_length=max_length,\n",
    "                                    top_k=top_k,\n",
    "                                    num_return_sequences=1)\n",
    "\n",
    "    generated_text = tokenizer.decode(sample_outputs[0].tolist())\n",
    "    return generated_text\n",
    "\n",
    "# Carregar o modelo e o tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
    "\n",
    "# Definir o comprimento m√°ximo de sequ√™ncia para 1024 tokens\n",
    "tokenizer.model_max_length = 1024\n",
    "\n",
    "# input sequence\n",
    "text = \"O mar salgado quanto de teu sal n√£o s√£o l√°grimas de Portugal\"\n",
    "generated_text = generate_text(model, tokenizer, text, max_length=150, top_k=40)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
